\section{Simulated Annealing}
Simulated annealing (SA) is a stochastic strategy to guide local-search heuristic algorithms towards the global optimum in problems of combinatorial optimization. In particular, it guides local-search heuristics -- which perturb a given solution to produce and evaluate solutions in its neighbourhood with respect to the perturbation, select a better solution, and repeat, eventually converging to a solution which is locally optimal -- towards the global optimum by prescribing random acceptance of solutions which are less optimal than the current solution at a decreasing rate which depends on the relative cost of the solutions. These suboptimal solutions enable the algorithm to ``jump'' out of local optima and explore more of the problem domain, and SA's acceptance probability is tightened according to a schedule which is designed such that the algorithm converges on the global optimum with high probability; or an acceptable optimum at a high rate, as desired.

SA was introduced by \textcite{Kirkpatrick671}, inspired by the Metropolis-Hastings algorithm (\cite{metro53}) of statistical mechanics, and modelled after the physical process of annealing. The essence of SA is the analogy between combinatorial optimization and the problem of determining the lowest-energy ground state configuration of a physical system; and though physically motivated, SA is purely a mathematical method and is almost universally applicable to combinatorial optimization problems.

There are three key components to a SA algorithm: the heuristic for the generation of new solutions from the current one, the probability of accepting a new solution based on its cost, and the tightening schedule (the ``annealing schedule'') of the acceptence probability. The first is problem-specific, and should aim for solutions with cost similar to the current solution in order to provide a smooth cost landscape; which eases convergence to the optimum, and is an important requirement of the physical and mathematical justification of the method (\cite{metaheurbook} give an excellent review of the latter). A simple and effective approach for the weighted set-covering problem is to remove sets at random, construct a trial solution by the greedy heuristic, and then cull any redundant columns; variations of this approach have been explored and validated by \cite{JacobsBrusco}. The probability of accepting a solution in SA is prescribed by the Boltzmann factor of statistical mechanics % essentially because it maximizes information entropy when nothing is known but the cost of solutions (insofar as the heuristic is unbiased) which helps the algorithm to explore
-- an exponential in the relative cost of solutions scaled by the ``temperature'' -- which is mathematically motivated by the Principle of Maximum Entropy and central to the Metropolis-Hastings algorithm; we'll probably leave this alone. \cite{qc174.8.m49} describe the possibility of extended definitions, but we've seen no such applications to the set covering problem. The annealing schedule provides the most room to play; it describes the overall rate, and manner -- linear, logarithmic, variant, \etc -- of change of the temperature, which affects both performance and running time in ways which can in general only be quantified and optimized through extensive benchmarking. We are particularly interested in implementing the Thermodynamic Simulated Annealing (TSA) method due \cite{thermoSA}, which dynamically determines the schedule according to physical law; a stretch of the physical analogy of SA which even allows the temperature to increase under certain conditions, and is largely self-tuning. The set covering problem would be a novel application of TSA.

Other important design considerations include initial solution selection and termination criteria. These topic will be explored as the project develops.
